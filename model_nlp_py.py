# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yCSn5_yc4E4jajnA9TNGGmMY_wLcjS1S
"""

import zipfile
local_zip = '/content/emotion_classify_data.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

import os
os.listdir()

import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv("/content/Emotion_classify_Data.csv")
df

df.isnull().sum()

import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download("stopwords")

def preprocess_text(text):
    text = str(text)
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\W+|\d+', ' ', text)
    word_tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [w for w in word_tokens if not w.lower() in stop_words]
    return " ".join(filtered_tokens)

df.Comment = df.Comment.apply(preprocess_text)
df

df.duplicated().sum()

df = df.drop_duplicates().reset_index(drop=True)
df.duplicated().sum()

df

from sklearn.model_selection import train_test_split
df_train, df_test = train_test_split(df, test_size=0.20, random_state=0)

df_train.Emotion.value_counts()

import matplotlib.pyplot as plt

df_train.groupby('Emotion').count().plot.bar(color='blue')

df_test.Emotion.value_counts()

df_test.groupby('Emotion').count().plot.bar(color='red')

category_train = pd.get_dummies(df_train.Emotion)
df_train_baru = pd.concat([df_train, category_train], axis=1)
df_train_baru = df_train_baru.drop(columns='Emotion')
df_train_baru

content_latih = df_train_baru['Comment'].values
label_latih = df_train_baru[['anger', 'fear', 'joy']].values

category_test = pd.get_dummies(df_test.Emotion)
df_test_baru = pd.concat([df_test, category_test], axis=1)
df_test_baru = df_test_baru.drop(columns='Emotion')
df_test_baru

content_test = df_test_baru['Comment'].values
label_test = df_test_baru[['anger', 'fear', 'joy']].values

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
max_tokens = 45

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(content_latih)

sekuens_latih = tokenizer.texts_to_sequences(content_latih)
sekuens_test = tokenizer.texts_to_sequences(content_test)

padded_latih = pad_sequences(sekuens_latih,
                             padding='post',
                             maxlen=max_tokens,
                             truncating='post')
padded_test = pad_sequences(sekuens_test,
                            padding='post',
                            maxlen=max_tokens,
                            truncating='post')

print(tokenizer.word_index)

print(padded_latih[:4])

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16, input_length=max_tokens),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(16),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.summary()
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

class ModelCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('accuracy') > 0.91 and logs.get('val_accuracy') > 0.91:
            self.model.stop_training = True

stop_callback = ModelCallback()
num_epochs = 250

history = model.fit(
    padded_latih,
    label_latih,
    epochs=num_epochs,
    validation_data=(padded_test, label_test),
    verbose=2,
    callbacks=stop_callback)

plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()

plt.plot(history.history['val_accuracy'])
plt.title('Model validation accuracy')
plt.ylabel('Validation Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()